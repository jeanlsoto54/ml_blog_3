[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jean Luis Soto’s Portfolio",
    "section": "",
    "text": "DELIVERY APP\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\ndelivery app orders prediction\n\n\n\n\n\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2023\n\n\nJean Luis Soto\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/ml/2023-12-19-delivery-app.html",
    "href": "posts/ml/2023-12-19-delivery-app.html",
    "title": "delivery app orders prediction",
    "section": "",
    "text": "# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Displaying at least 50 columns\npd.set_option('display.max_columns',50)\n\n\n# Load data file\ndf = pd.read_csv('C:\\\\Users\\\\equipo\\\\Downloads\\\\DETALLE ORDENES CANCELADAS_CL.csv')\ndf1 = pd.read_csv('C:\\\\Users\\\\equipo\\\\Downloads\\\\DETALLE ORDENES CANCELADAS_EC.csv')\ndf2 = pd.read_csv('C:\\\\Users\\\\equipo\\\\Downloads\\\\DETALLE ORDENES CANCELADAS_PE.csv')\n\n\n#Merging the datasets for the three countries Chile, Ecuador and Peru\nmerged_df = pd.concat([df, df1, df2], axis=0, ignore_index=True)\ndf = merged_df\n\nIt is being upleaded the dataset in 3 batches. each file contains the data of courier orders for the major cities of Perù, Chile and Ecuador\nThe dataset is composed from the next variables:\nCOUNTRY: Origin of the order (CL:chile,EC:ecuador,PE:peru)\nMZ_NAME: microzone of a city defined by the company where it ,mostly all the time, the store and the user.\nDATE_AGG: Date of the order\nDIA_NOMBRE: label of the day when the order happened\nREPURCHASE: Indicades, whether the order had no trackback (previous order for the same user the cancelled and order again in about 1 hour)‘With repurchase’ or not ‘Without repurchase’\nORDER_ID: code of the order\nUSER_TOTAL_ORDERS: counting of how many orders the user previously did\n% COMPLETED: % of the orders that were delivered to the client.\nPAYMENT_METHOD: method to pay the order.\nGMV_USD: net value of the order\nITEM_COUNT: number of items that compose the market basket\nCITY:\nBRAND_NAME:\nSTORE_ID:\nLNG:\nLAT\nT03_PRE_PICKING_TIME: Lead time that is about between the order being set and that the clerk starts to pick up the products.\nT04_PICKING_TIME: time that the clerk takes to gather all the items of the order\nT05_CASHIER_TIME: Time take by the cahier to generate the bill\nT06_REQUESTING_RT_TIME: Time that the clerk takes to ask for a courier to pick up the basket.\nT07_WAITING_RT_TIME: interval of time between the requested call for a courier and the arrivel of it.\nT08_CHECKOUT_TIME: Time that takes that the courier to put the basket on the vehicle\nVEHICLE_TYPE: Type of vehicle thatthe couries has\nMALL: Indicates if the the store is located on a shopping mall\nSUB_VERTICAL: Classification defined by the company for the store (Supermarket, pharmacy and so on)\nDISTANCE_KM: Distance between the store and the user\nTAG_GLOBAL_OFFER: Boolean that indicades if the order had a discount or not.\nSTOCKOUTS: number of Item (product id?) that were reported to be out of stock in the store.\nTAG_BO: Boolean that indicates if the order was labeled as bad order.\nTAG_BO_CANCEL: Indicades if the order was cancelled or not.\n\ndf.tail()\n\n\n\n\n\n\n\n\nCOUNTRY\nMZ_NAME\nDATE_AGG\nDIA_NOMBRE\nORDER_ID\nUSER_TOTAL_ORDERS\n% Completed orders user\nPAYMENT_METHOD\nGMV_USD\nITEM_COUNT\n...\nVEHICLE_TYPE\nSUB_VERTICAL\nSTOCKOUTS\nTAG_BO\nTAG_CANCEL\nDISTANCE_KM\nUSE_CREDIT\nHORA\nPOLYGON_SIZE\nRAPPI_AMOUNT\n\n\n\n\n62377\nPE\nMicrozona Bellavista\n2023-07-01 00:00:00\nsábado\n2160362847\n2\n100.00 %\ncash\n3.852632\n2.0\n...\nMotorcycle\nExpress\n0\nFalse\nFalse\n3.0\nNaN\n15\n6.0\nNaN\n\n\n62378\nPE\nJesús Maria - Lince LIM1\n2023-07-01 00:00:00\nsábado\n2160362853\n480\n98.13 %\ncc-DEBIT\n7.550000\n3.0\n...\nMotorcycle\nExpress\n0\nFalse\nFalse\n3.0\nNaN\n15\n8.0\nNaN\n\n\n62379\nPE\nSan Isidro Residencial LIM2\n2023-07-01 00:00:00\nsábado\n2160362870\n160\n97.50 %\ncc-credit\n14.089474\n1.0\n...\nBicycle\nRegalos\n0\nTrue\nFalse\n2.0\nNaN\n15\n2.0\nNaN\n\n\n62380\nPE\nChacarilla LIM2\n2023-07-01 00:00:00\nsábado\n2160362902\n1116\n92.38 %\ncc-credit\n22.402632\n3.0\n...\nMotorcycle\nFarmacia\n0\nFalse\nFalse\n1.0\nNaN\n15\n2.0\nNaN\n\n\n62381\nPE\nMiraflores Oeste LIM3\n2023-07-01 00:00:00\nsábado\n2160362916\n248\n97.98 %\ncc-credit\n11.697368\n5.0\n...\nMotorcycle\nFarmacia\n0\nFalse\nFalse\n1.0\nNaN\n15\n1.0\nNaN\n\n\n\n\n5 rows × 27 columns\n\n\n\n\ndf.info()\n\n\ndf.isnull().sum()\n\n\ndf.describe()\n\n\n2. Exploratory Data Analysis (EDA) (univariate / bivariate analysis)\n\n\nTranslation of the dataset from Spanish to English\n\nDIA_NOMBRE: Keep the name of the days when the order was\n\n\n# Checking unique values\ndf['DIA_NOMBRE'].unique()\n\narray(['viernes', 'martes', 'miércoles', 'lunes', 'jueves', 'sábado',\n       'domingo'], dtype=object)\n\n\n\n# Translating the Spanish values\ntrans = {'sábado': 'Saturday', 'lunes':'Monday', 'jueves':'Thursday', 'martes':'Tuesday', 'viernes':'Friday', 'miércoles':'Wednesday',\n       'domingo':'Sunday'}\ndf['DIA_NOMBRE'] = df['DIA_NOMBRE'].map(trans)\n\n\n# Lower case of the columns\ndf.columns = df.columns.str.lower()\n\n\n# Checking the values\ndf['sub_vertical'].unique()\n\narray(['Licores', 'Floristeria', 'Super', 'Express', 'Hogar',\n       'Bebes y niños', 'Tecnologia', 'Mascotas', 'Sex shop', 'Deportes',\n       'Farmacia', 'Belleza', 'Especializada', 'Smoking shop', 'Regalos',\n       'Libreria', 'Moda', 'Papeleria', 'TurboX', 'Jugueteria', 'Marcas'],\n      dtype=object)\n\n\nThe variable ‘SUB_VERTICAL’ contains the business sector to which is related the store. In this case, the labels are translated and compacted in fewer classes.\n\n# Translating the Spanish values\ntrans1 = {'Super' : 'Supermarket', 'Floristeria':'E-commerce', 'Express':'Convenience Store', 'Especializada': 'Special Convenient Store', 'Moda':'E-commerce',\n       'Libreria':'E-commerce', 'Tecnologia':'E-commerce', 'Hogar':'E-commerce', 'Farmacia':'Pharmacy', 'Mascotas':'E-commerce',\n       'Smoking shop':'E-commerce', 'Belleza':'E-commerce', 'Regalos':'E-commerce', 'Licores':'Licours Shop', 'Deportes':'E-commerce',\n       'Bebes y niños':'E-commerce', 'Jugueteria':'E-commerce', 'Sex shop':'E-commerce', 'TurboX':'E-commerce', 'Papeleria':'E-commerce',\n       'Outlet':'E-commerce'}\ndf['sub_vertical'] = df['sub_vertical'].map(trans1)\n\n\ndf = df.rename(columns={'sub_vertical':'store_category','mz_name':'village_name','date_agg':'date','dia_nombre':'weekday','gmv_usd':'order_price','stockouts':'out_of_stock','tag_bo':'wrong_order','tag_bo_cancel':'order_canceled'\n                        ,'polygon_size':'coberture area','rappi_amount':'cashback_amount'})\n\n\ndf.head()\n\n\n\n\n\n\n\n\ncountry\nvillage_name\ndate\nweekday\norder_id\nuser_total_orders\n% completed orders user\npayment_method\norder_price\nitem_count\ncity\nbrand_group\nlat\nlng\nt04_picking_time\nt05_cashier_time\nt06_requesting_rt_time\nvehicle_type\nstore_category\nout_of_stock\nwrong_order\ntag_cancel\ndistance_km\nuse_credit\nhora\ncoberture area\ncashback_amount\n\n\n\n\n0\nCL\nCampus Oriente SCL4\n2023-06-30 00:00:00\nFriday\n88222630\n111\n95.50 %\nrappi_pay_gateway\n25.407407\n2.0\nSantiago de Chile\nBotillería Echenique\n-33.433491\n-70.581792\nNaN\n14187.0\nNaN\nBicycle\nLicours Shop\n0\nFalse\nFalse\n2.0\nNaN\n23\nNaN\nNaN\n\n\n1\nCL\n23 Senador Jaime Guzman\n2023-06-27 00:00:00\nTuesday\n88256140\n4\n50.00 %\ncc-N/A\n43.049383\n1.0\nSantiago de Chile\nSOLOROSASYALGOMAS\n-33.403637\n-70.657823\n2.0\n0.0\nNaN\nMotorcycle\nE-commerce\n0\nTrue\nTrue\n3.0\nNaN\n0\nNaN\nNaN\n\n\n2\nCL\nLas Condes Poniente SCL3\n2023-06-27 00:00:00\nTuesday\n88287753\n208\n99.04 %\ncc-credit\n119.438889\n11.0\nSantiago de Chile\nJumbo\n-33.402225\n-70.598952\n8.0\n12.0\n1.0\nMotorcycle\nSupermarket\n0\nFalse\nFalse\n4.0\nNaN\n12\nNaN\nNaN\n\n\n3\nCL\n20 Pedro Fontova\n2023-06-27 00:00:00\nTuesday\n88290753\n405\n95.31 %\ncc-N/A\n13.061728\n1.0\nSantiago de Chile\nLider\n-33.380700\n-70.686793\nNaN\n0.0\nNaN\nNaN\nSupermarket\n0\nTrue\nTrue\n2.0\nNaN\n13\nNaN\nNaN\n\n\n4\nCL\nCampus Oriente SCL4\n2023-06-27 00:00:00\nTuesday\n88291146\n455\n96.92 %\ncc-N/A\n36.740741\n3.0\nSantiago de Chile\nJardineria teresa jacqueline huenun espinoza\n-33.449619\n-70.635816\nNaN\n0.0\nNaN\nMotorcycle\nE-commerce\n0\nTrue\nTrue\n4.0\nNaN\n13\nNaN\nNaN\n\n\n\n\n\n\n\nThe variables ORDER_ID and DATE are being dropped since these data is most useful in traceability than predictibility\n\n# Dropping unecessary columns\ndf.drop(['order_id'], axis= 1,inplace= True)\n\nThe data checked present a lot of variables with blank data. The next strategies are going to be implemented on the data:\n\nImputing the mean or mode value for the blank values: For the variables ‘STORE_CATEGORY’,‘VEHICLE_TYPE’ Since, in this categories,the is an actual probability that the values are related to the median or mode.\nDrop rows of the null values: ‘LAT’,‘LNG’,‘POLYGON_SIZE’. The number of rows are low and can be dropped.\nPut the fixed value ‘FALSE’ in ‘USE_CREDIT’: Since this is a boolean variable, and the blanks values mean False\nPut the fixed value of 0: For the variables T04,T05,T06, and rappi_amount because these variabkes are time variables and currency, in which if they don’t have value means that for that field, it doesn’t apply the variable.\nFinally, the variable % completed orders is in varchar and is going to be changed to decimal value\n\n\n# Checking null values\ndf.isnull().sum()\n\n\ndf.info()\n\n\ndf[['vehicle_type', 'store_category', 'distance_km', 'out_of_stock']].info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 62382 entries, 0 to 62381\nData columns (total 4 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   vehicle_type    61388 non-null  object \n 1   store_category  61964 non-null  object \n 2   distance_km     62380 non-null  float64\n 3   out_of_stock    62382 non-null  int64  \ndtypes: float64(1), int64(1), object(2)\nmemory usage: 1.9+ MB\n\n\n\n# Removing % label\ndf['% completed orders user']=df['% completed orders user'].str.replace('%',' ')\n\ndf['% completed orders user']=df['% completed orders user'].astype('float')\ndf['% completed orders user']=df['% completed orders user']/100\n\n\ndf[['% completed orders user']].info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 62382 entries, 0 to 62381\nData columns (total 1 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   % completed orders user  62382 non-null  float64\ndtypes: float64(1)\nmemory usage: 487.5 KB\n\n\n\n# Remove missing values\ndf.dropna(subset=['lat','lng','vehicle_type'],inplace=True)\n\n\n#Imputing values of mode and mean for the variables\n\nmode_ps = df['polygon_size'].mode()[0]\ndf['polygon_size'].fillna(mode_ps, inplace=True)\n\n\nmean_ic = df['item_count'].mean()\ndf['item_count'].fillna(mean_ic, inplace=True)\n\n\nmode_sc = df['store_category'].mode()[0]\ndf['store_category'].fillna(mode_sc, inplace=True)\n\n\n# Replace missing values\ndf.fillna({'t04_picking_time': 0, 't05_cashier_time': 0, 't06_requesting_rt_time':0, 'rappi_amount':0}, inplace=True)\n\n\n# Replace missing values\ndf.fillna({'use_credit': False},inplace=True)\n\n\nzero_count = (df['order_value_usd'] == 0).sum()\nprint(zero_count)\n\n\n# Check duplicate values\ndup_value = df.duplicated().sum()\nprint(dup_value)\n\n\ndf.isnull().sum()\n\n\ndf.describe()\n\n\ndf.head()\n\n\n# Removing negative values\nnegative_cols = ['user_total_orders', 'item_count', 't04_picking_time', 't05_cashier_time', 't06_requesting_rt_time',\n                 'out_of_stock', 'distance_km', 'hora', 'polygon_size', 'rappi_amount']\n\n# Create a mask that identifies rows with any negative value in the specified columns\nnegative_mask = df[negative_cols].lt(0).any(axis=1)\n\n# Use the mask to filter out rows with negative values\ndf1 = df[~negative_mask]\n\n\n# Separating quantiative & Categorical variables\ndf_cat = df.select_dtypes(include=['object','bool'])\ndf_quan = df.select_dtypes(exclude=['object','bool'])\n\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\ndf_quan_corr = df_quan.corr(method = 'spearman' )\nax = sns.heatmap(df_quan_corr, annot=True, cmap='coolwarm', center=0,fmt='.2g',xticklabels='auto', yticklabels='auto',linewidth=.8,cbar_kws={\"shrink\": .8})\nax.tick_params(axis='both', which='both', labelsize=8)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n# Create a pairplot\nsns.pairplot(df_quan)\n\n\nfor col in df_quan.columns:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\n    # Histogram\n    sns.histplot(df_quan[col], kde=True, bins=20, ax=ax[0])\n    ax[0].set_title(f'Histogram for {col}')\n    ax[0].set_xlabel(col)\n    ax[0].set_ylabel('Frequency')\n\n    # Box plot\n    sns.boxplot(x=df_quan[col], ax=ax[1])\n    ax[1].set_title(f'Boxplot for {col}')\n    ax[1].set_xlabel(col)\n\n    plt.tight_layout()\n    plt.show()\n\n\ndf_quan.columns\n\n\n#Removing outliers for : % completed orders user, picking_time, cashier_time, t_06, polygon size, rapi_amount\ncolumns = ['t04_picking_time', 't05_cashier_time',\n           't06_requesting_rt_time', 'polygon_size', 'rappi_amount']\n\nQ1 = df[columns].quantile(0.25)\nQ3 = df[columns].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = pd.DataFrame()\nfor col in columns:\n    condition = (df[col] &lt; lower_bound[col]) | (df[col] &gt; upper_bound[col])\n    outliers = outliers.append(df[condition], ignore_index=True)\n\n\n#chi_squared for categorical\nfrom scipy.stats import chi2_contingency\n\nchi_squared_results = pd.DataFrame(columns=['Variable 1', 'Variable 2', 'Chi-Squared', 'P-Value'])\n\nfor var1 in df_cat:\n    for var2 in df_cat:\n        if var1 != var2:\n          #if var2 =='bad_order':\n            contingency_table = pd.crosstab(df[var1], df[var2])\n\n            chi2, p, _, _ = chi2_contingency(contingency_table)\n\n\n            new_row = pd.DataFrame({'Variable 1': [var1], 'Variable 2': [var2], 'Chi-Squared': [chi2], 'P-Value': [p]})\n\n            chi_squared_results = pd.concat([chi_squared_results, new_row], ignore_index=True)\n\nchi_squared_results\n\n\n#Checking for independance between our outcome bad_order and the categorical variables\n\n\n#chi_squared for categorical\nfrom scipy.stats import chi2_contingency\n\nchi_squared_results = pd.DataFrame(columns=['Variable', 'Chi-Squared', 'P-Value'])\n\noutcome_variable = 'bad_order'\n\nfor var in df_cat:\n        if var != outcome_variable:\n            contingency_table = pd.crosstab(df[outcome_variable], df[var])\n\n            chi2, p, _, _ = chi2_contingency(contingency_table)\n\n\n            new_row = pd.DataFrame({'Variable': var, 'Chi-Squared': [chi2], 'P-Value': [p]})\n\n            chi_squared_results = pd.concat([chi_squared_results, new_row], ignore_index=True)\n\n            if p &lt; 0.05:\n              print(f\"There is a significant association between {outcome_variable} and {var} (p-value = {p:.2f})\")\n            else:\n              print(f\"There is no significant association between {outcome_variable} and {var} (p-value = {p:.2f})\")\n\n#H0 there is no relathionship between the variables\n\nchi_squared_results\n\n\n#Perfoming ANOVA TEST for mixed categorical and numerical values\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\n# List of numerical variables for the ANOVA tests\ndf_cuantitative = ['% completed orders user', 'order_value_usd', 'item_count', 't04_picking_time', 't05_cashier_time', 't06_requesting_rt_time', 'out_of_stock', 'distance_km', 'hora']\n\n# Perform ANOVA for numerical variables\nfor num_var in df_cuantitative:\n    formula = f'{num_var} ~ C(bad_order)'\n    try:\n        model = ols(formula, data=df).fit()\n        anova_table = anova_lm(model)\n        print(f\"ANOVA table for {num_var} and bad_order:\")\n        print(anova_table)\n\n        p_value = anova_table['PR(&gt;F)'][0]\n        if p_value &lt; 0.05:\n            print(f\"There is a significant difference between the means of the groups defined by bad_order for {num_var} (p-value = {p_value:.2f})\")\n        else:\n            print(f\"There is no difference between the means of k groups\")\n\n        print(\"=\" * 40)\n\n    except Exception as e:\n        print(f\"Error for {num_var} and bad_order: {str(e)}\")\n\n\n\n\n\n\n4. Machine Learning\n\n# Feature engineering\ndf_quan\n\nFor this part we are going to start put the data in a standarization and onehot encoding so the data is ready to be inserted in the modeling. For it, we are going to use the Pipeline method\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass VariableSelector(BaseEstimator, TransformerMixin):\n    def __init__(self,attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X , y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ncolumns_quan = list(df_quan.columns)\n\n\ncolumns_quan\n\n\n#quantitative\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nquanti_pipeline = Pipeline(\n    [\n     ('selector', VariableSelector(columns_quan)),\n     ('std_scaler', StandardScaler())\n    ]\n    )\n\n\n#It is being checked in the case of the categorical variables, which values can be apprved for a OHE method or label encoding\n\ndf_cat_oh = ['country','payment_method','city','vehicle_type','store_category','use_credit']\ndf_cat_le = ['district_name','brand_group']\n\n\nfrom sklearn.preprocessing import OneHotEncoder as OHE\n\ncuali_pipeline_ohe = Pipeline(\n\n  [\n    ('selector',VariableSelector(df_cat_oh)),\n    ('OHE_columns',OHE(sparse=False))\n\n  ]\n\n)\n\n\nfrom sklearn.preprocessing import OrdinalEncoder as OE\n\ncuali_pipeline_le =Pipeline(\n\n                            [\n                             ('selector',VariableSelector(df_cat_le)),\n                             ('LE_columns', OE())\n                            ]\n\n)\n\n\nfrom sklearn.pipeline import FeatureUnion\n\nfull_pipeline =FeatureUnion(\n    transformer_list=[\n        ('num_pipeline', quanti_pipeline),\n        ('ohe_pipeline',cuali_pipeline_ohe),\n        ('le_pipeline',cuali_pipeline_le)\n                      ]\n)\n\nOnce that is defined the Pipeline, to do the adjustements on the data. It is going to be retrieved from the main dataset the y to be predicted\n\nx_df = full_pipeline.fit_transform(df)\n\n\nX = x_df\ny = df['bad_order']\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2,random_state=42,shuffle=True,stratify = y)\n\nNow that we splited our data, we will apply 3 models on our categorical outcome (Logistic regression, Classification Tree, Random Forest)\n\nLOGISTICS REGRESSION\n\n\n#Logistic Regression Model Fitting\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(penalty = None, solver = 'newton-cholesky')\nlogreg.fit(X_train, y_train)\n\n#Predicting the test set results\nlr_pred = logreg.predict(X_test)\n\n\n#Calculating the accuracy\nprint('Accuracy of Logistic Regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n\n\n#Checking crossvalidated performance\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(logreg, X_test, y_test)\nprint(f\"average cross validated score : {np.mean(cv_score)}\")\n\n\n#Logistic Regression model performance with Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nconfusion_matrix_lr = confusion_matrix(y_test, lr_pred, labels= logreg.classes_)\nConfusionMatrixDisplay(confusion_matrix_lr, display_labels = logreg.classes_).plot()\nprint(classification_report(y_test,lr_pred))\n\n\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score, f1_score,roc_auc_score,cohen_kappa_score,roc_curve\naccuracy_lr = accuracy_score(y_test, lr_pred)\nprecision_lr = precision_score(y_test, lr_pred)\nsensitivity_lr = recall_score(y_test, lr_pred)\nf1_lr = f1_score(y_test, lr_pred)\n\n#ROC Curve\nlr_pred_prob= logreg.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, lr_pred_prob[:,1])\nlr_roc_auc = roc_auc_score(y_test, lr_pred_prob[:,1])\n\n\nresult_lr = pd.DataFrame([['Log. reg.',accuracy_lr, precision_lr, sensitivity_lr,f1_lr,lr_roc_auc]], columns=('model','accuracy','precision','sensitivity','f1','roc'))\nresult_lr\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=' Logit Regression - ROC curve (area = {:.2f})'.format(lr_roc_auc))\nplt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\nplt.xlim([0.0, 1])\nplt.ylim([0.0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower center')\nplt.show()\n\n\nDecision Tree\n\n\n#Classification Tree Model Fitting\n\nfrom sklearn.tree import DecisionTreeClassifier\n# Initializing Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf.fit(X_train,y_train)\n\n#Predicting on the test set\nclf_pred = clf.predict(X_test)\n\n\n#Calculating the model accuracy, how often is the classifier correct ?\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Decision Tree Classifier: {:.2f}\".format(accuracy_score(y_test, clf_pred)))\n\n\n#Classification Tree Model performance\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nconfusion_matrix_clf = confusion_matrix(y_test, clf_pred, labels = clf.classes_)\nConfusionMatrixDisplay(confusion_matrix_clf, display_labels= clf.classes_).plot()\nprint(classification_report(y_test, clf_pred))\n\n\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(\n clf,\n out_file=(\"order_simple.dot\"),\n feature_names=None,\n class_names=None,\n filled=True,\n)\n\n\n!dot -Tpng hitters_simple.dot -o hitters_simple.png\n\n\nfrom IPython.display import Image\nImage(\"order_simple.png\")\n\n\naccuracy_clf = accuracy_score(y_test, clf_pred)\nprecision_clf = precision_score(y_test, clf_pred)\nsensitivity_clf = recall_score(y_test, clf_pred)\nf1_clf = f1_score(y_test, clf_pred)\n\n\n#ROC Curve\nclf_pred_prob= clf.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, clf_pred_prob[:,1])\nclf_roc_auc = roc_auc_score(y_test, clf_pred_prob[:,1])\n\n\nresult_clf = pd.DataFrame([['decision tree',accuracy_clf, precision_clf, sensitivity_clf,f1_clf,clf_roc_auc]], columns=('model','accuracy','precision','sensitivity','f1','roc'))\nresult_clf\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label='Tree Dec. -ROC curve (area = {:.2f})'.format(clf_roc_auc))\nplt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\nplt.xlim([0.0, 1])\nplt.ylim([0.0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower center')\nplt.show()\n\n\nRandom Forest\n\n\n#Random Forest Model Fitting\n\nfrom sklearn.ensemble import RandomForestClassifier\n# Initializing Random Forest Classifer object\nrf = RandomForestClassifier()\n\n# Train Random Forest Classifer\nrf.fit(X_train, y_train)\n\n#Predicting on the test set\nrf_pred = rf.predict(X_test)\n\n\n#Calculating Random Forest model accuracy\naccuracy_rf = accuracy_score(y_test, rf_pred)\nprint(\"Accuracy Random Forest Classifier: {:.2f}\".format(accuracy_rf))\n\n\n#Random Forest Classifier model performance\nconfusion_matrix_rf = confusion_matrix(y_test, rf_pred, labels = rf.classes_)\nConfusionMatrixDisplay(confusion_matrix_rf, display_labels= rf.classes_).plot()\nprint(classification_report(y_test, rf_pred))\n\n\naccuracy_rf = accuracy_score(y_test, rf_pred)\nprecision_rf = precision_score(y_test, rf_pred)\nsensitivity_rf = recall_score(y_test, rf_pred)\nf1_rf = f1_score(y_test, rf_pred)\n\n\n#ROC Curve\nrf_pred_prob= rf.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, rf_pred_prob[:,1])\nrf_roc_auc = roc_auc_score(y_test, rf_pred_prob[:,1])\n\nresult_rf = pd.DataFrame([['random forest',accuracy_rf, precision_rf, sensitivity_rf,f1_rf,rf_roc_auc]], columns=('model','accuracy','precision','sensitivity','f1','roc'))\nresult_rf\n\nAfter checking model performance accuracy (counts of correct classification) we can assume that our best model is Random Forest Classifier with 0.93% of correctly predicted predictions\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label='Rand. forest -ROC curve (area = {:.2f})'.format(rf_roc_auc))\nplt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\nplt.xlim([0.0, 1])\nplt.ylim([0.0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc='lower center')\nplt.show()\n\n\nresultados = pd.concat([result_lr, result_clf,result_rf], axis=0)\nresultados\n\nResults:\n\nMetrics\n\n\nOverall the accuracy between all the models seems to be high (between 86 and 92%) which meant that the models have been a great deal into predicting if the order was labeled as a good one or bad one.\nPrecision: However, for this exercise it is most interesting to predict only the bad orders, to which this metric is most important. As it can be revised, the random forest model is the one with the best result out of the three. Which means that of the total predicted true bad orders, the model have an 86% of good prediction\nSensitivity: However, all the models had a bad ratio in this metric. This metric indicates that the of all the True orders labeled as bad order, only ;in the best case; was predicted 30% of this cases. To make the model more fit, it will be used the methods for reduce the overfitting of the models.\n\n\n\nTuning model\n\nestimators = list(range(2, 50, 5))\nsens_rf_num_estimators = []\naccu_rf_num_estimators = []\n\nfor n in estimators:\n    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_test)\n    sensitivity_rf = recall_score(y_test, rf_pred)\n    accuracy_rf = accuracy_score(y_test, rf_pred)\n\n    sens_rf_num_estimators.append(sensitivity_rf)\n    accu_rf_num_estimators.append(accuracy_rf)\n\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.plot(estimators, sens_rf_num_estimators, marker='o', linestyle='-', color='b')\nplt.plot(estimators,accu_rf_num_estimators, marker='x', linestyle='-', color='r')\nplt.title('Sensitivity vs. Number of Estimators')\nplt.xlabel('Number of Estimators')\nplt.ylabel('%')\nplt.grid(True)\nplt.show()\n\n\ntrain_scores = []\nvalid_scores = []\nleaves = list(range(2,30))\nfor leaf in leaves :\n    dtr = RandomForestClassifier(min_samples_leaf = leaf)\n    dtr.fit(X_train, y_train)\n    train_scores.append(dtr.score(X_train, y_train))\n    valid_scores.append(dtr.score(X_test, y_test))\n\nplt.figure(figsize=(10,8))\nplt.plot(leaves, train_scores)\nplt.plot(leaves, valid_scores)\nplt.xlabel(\"minimum samples per leaf\")\nplt.ylabel(\"score values\")\nplt.legend([\"training scores\", \"validation scores\"])\nplt.show()\n\n\ntrain_scores = []\nvalid_scores = []\ndepths = list(range(2,40))\nfor depth in depths :\n    dtr = RandomForestClassifier(max_depth = depth)\n    dtr.fit(X_train, y_train)\n    train_scores.append(dtr.score(X_train, y_train))\n    valid_scores.append(dtr.score(X_test, y_test))\n\nplt.figure(figsize=(10,8))\nplt.plot(depths, train_scores)\nplt.plot(depths, valid_scores)\nplt.xlabel(\"maximum depth\")\nplt.ylabel(\"score values\")\nplt.legend([\"training scores\", \"validation scores\"])\nplt.show()\n\n\nGridSearchCV\n\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'max_depth': list(range(2, 15)), 'min_samples_leaf': list(range(2,20))}\n\ngrid_search_cv = GridSearchCV(RandomForestClassifier(),\n                              params,\n                              verbose=1,\n                              cv=3,\n                              n_jobs=-1)\n\ngrid_search_cv.fit(X_train, y_train)\n\n\nbest_tree_model = grid_search_cv.best_estimator_\nbest_tree_model\n\n\nbest_tree_model.fit(X_train,y_train)\nbest_tree_model.score(X_test,y_test)\n\n\nrfo_pred = best_tree_model.predict(X_test)\n\n\nfrom sklearn.tree import export_graphviz\ntree_to_visualize = best_tree_model.estimators_[0]\n\nexport_graphviz(\n    tree_to_visualize,\n    out_file=(\"optimal_tree.dot\"),\n    feature_names=None,  # Specify your feature names if needed\n    class_names=None,    # Specify class names for classification tasks\n    filled=True\n)\n\n\n!dot -Tpng hitters_simple.dot -o hitters_simple.png\nImage(\"optimal_tree.png\")\n\n\naccuracy_rfo = accuracy_score(y_test, rfo_pred)\nprecision_rfo = precision_score(y_test, rfo_pred)\nsensitivity_rfo = recall_score(y_test, rfo_pred)\nf1_rfo = f1_score(y_test, rfo_pred)\n\n\nresult_rfo = pd.DataFrame([['optimal random forest',accuracy_rfo, precision_rfo, sensitivity_rfo,f1_rfo]], columns=('model','accuracy','precision','sensitivity','f1'))\nresult_rfo\n\n\n\nUnsupervised Learning\n\n'''Importing the needed libraries and getting a basic idea on the df_quant to see what we are looking at'''\ntry:\n    from fanalysis.pca import PCA\nexcept:\n    !pip install fanalysis\n    from fanalysis.pca import PCA\ndisplay(df[df_quan].head(3))\ndisplay(df[df_quan].info())\np = D.shape[1]\nn = D.shape[0]\nD = df[df_quan]\nX = D.values\n\n\n'''Step 1: Try PCA to reduce dimensionality'''\n# instantiate acp object form PCA class\nacp = PCA(std_unit=True,row_labels=D.index,col_labels=D.columns) #std_unit=True, doing standardized PCA)\n\n# run PCA on X observed data\ntry:\n    acp.fit(X)  # This will raise a ZeroDivisionError\nexcept Exception as e:\n    display(f\"An error occurred: {e}\")\n\n\n'''Step 2: Trouble shooting the error, which may indicate that there are columns that are zero'''\nD_corr = D.corr()\nax = sns.heatmap(D_corr, annot=True, cmap='coolwarm', center=0,fmt='.2g',xticklabels='auto', yticklabels='auto',linewidth=.8,cbar_kws={\"shrink\": .8})\nax.tick_params(axis='both', which='both', labelsize=8)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n'''\nStep 3: Issue becomes clear that the 't06_requesting_rt_time' is the problematic one because apparently\nafter data cleaning, all the values we kept are zero, which makes sense given the value means the time it took for\na delivery personnel to be assigned to the order, which is expected to be swifty in any delivery app.\n\nSolution here: get rid off such column and do it again\n'''\ndisplay(D.columns[5])\nD1 = D.drop(columns = [D.columns[5]])\nX1 = D1.values\np1 = D1.shape[1]\nn1 = D1.shape[0]\nacp = PCA(std_unit=True,row_labels=D1.index,col_labels=D1.columns) #std_unit=True, doing standardized PCA)\nacp.fit(X1)  # This will raise a ZeroDivisionError\ndisplay(acp.col_labels)\ndisplay(acp.eig_) # each lamda k is in the first array, and then each lamda k divided by the sum of lamda k\n#(proportion of variance explained by k),\n# aka the variance explained by each component, is in the second row.\n# The third row is the cumulative variance.\n'''Looking at the cumulative variance, the most variable with the most variance only took 22%\nof the total variance in the quantitative columns'''\ndisplay(acp.eig_.shape)\n\n\n'''Step 4: determining the threshold'''\n\n# first the main plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nax[0].plot(range(1,p1+1),acp.eig_[0],\".-\")\nax[0].set_xlabel(\"Nb. of factors\")\nax[0].set_ylabel(\"Eigenvalues\")\nax[0].set_title(\"Scree Plot\")\n\n# add Kaiser's threshold line\nax[0].plot([1,p1+3],[1,1],\"r--\",linewidth=1)\n\n# print explained variance plot\n\n\nax[1].plot(range(0,p1+1),np.append(0,acp.eig_[2]),\".-\")\nax[1].set_xlabel(\"Nb. of factors\")\nax[1].set_ylabel(\"% of explained variance\")\nax[1].set_title(\"Explained Variance\")\n\n'''According to the plot, I decided to choose 4 as the threshold'''\n\n\n# Applying Barlett's test of Sphericity\ntry:\n    from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nexcept:\n    !pip install factor_analyzer\n    from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n\nchi_square_value, p_value = calculate_bartlett_sphericity(X1)\nprint(chi_square_value, p_value)\n\n\n# Computing Karlis-Saporta-Spinaki threshold\n# impor math package\nimport math\n\n#seuil de Karlis-Saporta-Spinaki\nkss = 1+2*math.sqrt((p1-1)/(n1-1))\nprint(f\"Karlis-Saporta-Spinaki threshold: {kss:.3f}\")\n\n\n# Broken sticks method\n# threshold for the broken sticks\nb = np.flip(np.cumsum(1/np.arange(p1,0,-1)))\nprint(f\"thresholds for the broken sticks: {b}\")\n\n# plot eigenvalues\nfig, ax = plt.subplots(figsize=(5,5))\nax.plot(range(1,p1+1),acp.eig_[0],\".-\")\nax.set_xlabel(\"Nb. if factors\")\nax.set_ylabel(\"Eigenvalues\")\nplt.title(\"Eigenvalues and broken sticks thresholds\")\n\n# add broken sticks thresholds\nax.plot(range(1,p1+1),b,\"r--\",linewidth=1)\n\nplt.show()\n\n\n'''Step 5: Variable representation'''\nprint(pd.DataFrame(acp.col_coord_[:,:4],index=D1.columns, columns=['F1','F2','F3','F4']))\n\n\n# Correlations circle\nacp.correlation_circle(num_x_axis=1,num_y_axis=2, figsize=(5,5))\nacp.correlation_circle(num_x_axis=3,num_y_axis=4, figsize=(5,5))\n'''Interpretation of the two dimensions'''\n'''Three variables have a quite negative correlation with Dim 1, which are item_count, picking_time\nand cashier_time. This means the speed the store can get those orders ready, which, unsurprisingly, also partially\ncomes down to the size of the order. There is clearly a size effect in this dimension'''\n'''Two variables have a quite negative correlation with Dim 2, which are distance_km and order_value_USD. This means\nthat whether the order is cheap and within a shorter distance, most likely being some everyday shopping needs versus\nsome more expensive order traveling a longer distance. This shows a shape effect while Dim 1 shows a size effect'''\n'''Two variables have a quite strong positive correlation with Dim 3, namely 'hora' which means hour, meaning the\ntime in the day, and 'out_of_stock'. This indicates that this axis talks about a general time variation, since the\nearly an order is placed, the less likely that the specific item a customer wants would be sold out.'''\n'''The fourth dimension has a very high correlation on '% completed orders user' which could just be interpreted as\nthat specific variable.'''\n\n\n# Cos² if the variables on the two first factors\nprint(pd.DataFrame(acp.col_cos2_[:,:4],index=D1.columns, columns=['F1','F2','F3','F4']))\n\n\n# Cumulated Cos² on the two first factors\nprint(pd.DataFrame(np.cumsum(acp.col_cos2_[:,:4],axis=1),index=D1.columns, columns=['F1','F2',\"F3\",'F4']))\n\n\n# Contributions of each variable on the two first factors (in %)\nprint(pd.DataFrame(acp.col_contrib_[:,:4],index=D1.columns, columns=['F1','F2',\"F3\",'F4']))\n\n\n# chart of the individuals\nacp.mapping_row(num_x_axis=1,num_y_axis=2,figsize=(7,7))\n\n\n# chart of the individuals\nacp.mapping_row(num_x_axis=3,num_y_axis=4,figsize=(7,7))\n'''From this factor map, we can see clearly that if we use Dim 3 and Dim 4 as our x1 and x2 axis and plot the data\npoints, there is a CLEAR tendency that different clusters are formed likely due to the variance introduced by\ndifferent rows, aka different groups of the customers behaving differently'''\n\n\n# creating a new data frame with the row coordinates of F1, F2, F3 and F4\ndf_row_coord = pd.DataFrame(np.cumsum(acp.row_coord_[:,:4],axis=1),index=D1.index, columns=['F1','F2','F3',\"F4\"])\ndf_row_coord.head(5)\n\n\nmask = df_row_coord['F1'] == df_row_coord['F1'].min()\ndf_row_coord.loc[df_row_coord[mask].index, :]\n\n\n'''Clustering'''\n'''Plotting them one axis by another first'''\nsns.pairplot(df_row_coord)\n\n\n'''Looking at the pairplot between four different dimensions, the pattern is obvious that in some of these pairs\nof features, two different cluster can be seen.'''\n\n\n#Import the library and initialization\nfrom sklearn.cluster import KMeans\n\nX = df_row_coord.values\nkm = KMeans(n_clusters = 2, init = 'k-means++' , n_init = 10, max_iter = 300, random_state = 0, tol = 0.0001)\ny_km = km.fit_predict(X)\n\n\nkm.labels_\nfrom collections import Counter\nunique_counts = Counter(km.labels_)\ndisplay(unique_counts)\n\n\n#Added the clustering info to the dataframe, but it won't affect the original X since X was done in the front as\n#X = df_row_coord.values\ndf_row_coord['Y_KMean'] = y_km\nsns.pairplot(df_row_coord, hue='Y_KMean')\n\n\nfrom sklearn.cluster import DBSCAN\neps_list = [0.2, 0.5, 1, 2, 2.2, 2.5, 2.7, 3, 4, 5]\nfor eps_ in eps_list:\n    DBSCAN_model = DBSCAN(eps = eps_, min_samples=5, metric='euclidean')\n    DBSCAN_model.fit_predict(X)\n    y_DBSCAN = DBSCAN_model.labels_\n    df_row_coord['Y_DBSCAN'] = y_DBSCAN\n    display(df_row_coord['Y_DBSCAN'].value_counts())\n    sns.pairplot(df_row_coord.drop(columns = ['Y_KMean']), hue='Y_DBSCAN')\n\n\nDBSCAN_model = DBSCAN(eps=4, min_samples=5, metric='euclidean')\nDBSCAN_model.fit_predict(X)\ny_DBSCAN = DBSCAN_model.labels_\ndf_row_coord['Y_DBSCAN'] = y_DBSCAN\ndisplay(df_row_coord['Y_DBSCAN'].value_counts())\nsns.pairplot(df_row_coord.drop(columns = ['Y_KMean']), hue='Y_DBSCAN')\n\n\ndf[df_row_coord['Y_DBSCAN'] == 1]\n\n\ndf_row_coord['Y_DBSCAN'].value_counts()"
  },
  {
    "objectID": "posts/ml/index.html",
    "href": "posts/ml/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/delivery/Delivery_app_group_project.html",
    "href": "posts/delivery/Delivery_app_group_project.html",
    "title": "DELIVERY APP",
    "section": "",
    "text": "The next exercise is related to the development of a machine learning modeling that have as a goal to classify if an order is being delivered correctly or not.\nThe dataset provided is from a ficticious company that provides the service of delivery from stores as grocery shops, pharmacies and more to their clients. The company have as a SLA the commitment to delivery successfully all the orders that they have, if happens otherwise the company is penalized since they need to assume the losses of the order that have being handled poorly.\nThe company has a general understanding on the problems that lead to have unsusscesful order. For example:\n\nThe product requested has gone stockout and the partner did not report it on time.\nThe high demand creates that there are some lack of delivery vehicles in certain hours.\nThe products arrive to the clients in a badly state.\nThe route to deliver the order presents bottlenecks (stores, distance and more) and the order is not handled on time.\n\nNow, the company knows from where it comes the problems. However, wants to automatize the problem of recognizing orders that are in danger to being wrong handled. For it, it is displayed a dataset of orders, the characteristics that are being picked in real time about the order and the result of it.\nIn the article, it is going to be explained the process to provide of an algorithm to classify and recognize the orders in real time and issue aids to avoid the losses of wrong orders.\n\n# Displaying at least 50 columns\npd.set_option('display.max_columns',50)\n\nFirstly, the next libraries are going be imported in the lab. + pandas and numpy to manage the numerical data + seaborn and matplotlib are oriented to create statistical graphs\n\n# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n#/ @hidden_cell\ndelivery = pd.read_csv('.//ml_dlv.csv')\n\nData\nNow is going to be presented the data about a company that has as goal to deliver different products to the clients in the countries that it operates. In this table there is a lot of data about the operations logistic of the company. Many od thiese data comes is real time from several APIs scrateched from applications of the partners, the clients, and the couriers. Which now in this dataset have been tangled.\n\n\n\n\n\n\n\nField\nConcept\n\n\n\n\ncountry\nThere are going to three countries in the dataset\n\n\ndistrict_name\nName of the village in the city\n\n\ndate\nday when the order was commanded\n\n\nweekday\nname of the day when the order was commanded\n\n\nuser_total_orders\nnumber of previous orders that the client had ordered before\n\n\ncompleted orders user\nPercentage of the orders that actually were well delivered to the client\n\n\npayment_method\nDefines the way in which the client pays for the order\n\n\norder_value\nvalue of the order\n\n\nitem_count\nnumber of elements that the basket of the order have\n\n\ncity\nlocation of the order\n\n\npartner\nStore or brand that provided the items requested by the client\n\n\npicking_time\nLead time that the clerk takes to gather all the items of the order.\n\n\ncashier_time\nTime take by the cashier to generate the bill\n\n\nrequesting_transport_time\nTime that the clerk takes to ask for a courier to pick up the basket.\n\n\nvehicle_type\nType of vehicle that the courier have\n\n\nstore_category\nMacro classification of the partner\n\n\nout_of_stock\nNumber of elements that were reported to be out of stock in the store.\n\n\norder_wrong\nIndicades if the order was sucessfully delivered or not\n\n\ntag_cancel\nIndicades if the order was cancelled or not.\n\n\ndistance_km\nTotal number of km that the delivery had from the partner to the client\n\n\nuse_credit\nIndicades if the order was payed with some promotion code\n\n\nhora\nHave the hour in which the order was issued\n\n\npolygon_size\nDefines the km of coverage of the partner\n\n\ncashback\nProvides the total amount of promotion code value in possess of the client\n\n\n\n\ndelivery.head(3)\n\n\n\n\n\n\n\n\ncountry\ndistrict_name\ndate\nweekday\nuser_total_orders\n% completed orders user\npayment_method\norder_value\nitem_count\ncity\npartner\npicking_time\ncashier_time\nrequesting_transport_time\nvehicle_type\nstore_category\nout_of_stock\norder_wrong\ntag_cancel\ndistance_km\nuse_credit\nhora\npolygon_size\ncashback\n\n\n\n\n0\nLlaqta_1\nMA_1\n2023-06-30 00:00:00\nFriday\n111\n95.50 %\nrevol_pay\n25.407407\n2.0\nMaule\nBotillería Echenique\nNaN\n14187.0\nNaN\nBicycle\nLicores\n0\nFalse\nFalse\n2.0\nNaN\n23\nNaN\nNaN\n\n\n1\nLlaqta_1\nMA_2\n2023-06-27 00:00:00\nTuesday\n4\n50.00 %\ncc-N/A\n43.049383\n1.0\nMaule\nSOLOROSASYALGOMAS\n2.0\n0.0\nNaN\nMotorcycle\nFloristeria\n0\nTrue\nTrue\n3.0\nNaN\n0\nNaN\nNaN\n\n\n2\nLlaqta_1\nMA_3\n2023-06-27 00:00:00\nTuesday\n208\n99.04 %\ncc-credit\n119.438889\n11.0\nMaule\nJumbo\n8.0\n12.0\n1.0\nMotorcycle\nSuper\n0\nFalse\nFalse\n4.0\nNaN\n12\nNaN\nNaN"
  },
  {
    "objectID": "posts/delivery/Delivery_app_group_project.html#preview",
    "href": "posts/delivery/Delivery_app_group_project.html#preview",
    "title": "1. Load dataset",
    "section": "Preview",
    "text": "Preview\nThe data checked present a lot of variables with blank data. The next strategies are going to be implemented on the data:\n\nImputing the mean or mode value for the blank values: For the variables ‘STORE_CATEGORY’,‘VEHICLE_TYPE’ Since, in this categories,the is an actual probability that the values are related to the median or mode.\nDrop rows of the null values: ‘LAT’,‘LNG’,‘POLYGON_SIZE’. The number of rows are low and can be dropped.\nPut the fixed value ‘FALSE’ in ‘USE_CREDIT’: Since this is a boolean variable, and the blanks values mean False\nPut the fixed value of 0: For the variables T04,T05,T06, and rappi_amount because these variabkes are time variables and currency, in which if they don’t have value means that for that field, it doesn’t apply the variable.\nFinally, the variable % completed orders is in varchar and is going to be changed to decimal value\n\n\n# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# @hidden_cell\ndelivery = pd.read_csv('.//ml_dlv_1.csv')\n\n\n\n\n\n\n\n\nField\nConcept\n\n\n\n\ncountry\nThere are going to three countries in the dataset\n\n\ndistrict_name\nName of the village in the city\n\n\ndate\nday when the order was commanded\n\n\nweekday\nname of the day when the order was commanded\n\n\nuser_total_orders\nnumber of previous orders that the client had ordered before\n\n\ncompleted orders user\nPercentage of the orders that actually were well delivered to the client\n\n\npayment_method\nDefines the way in which the client pays for the order\n\n\norder_value\nvalue of the order\n\n\nitem_count\nnumber of elements that the basket of the order have\n\n\ncity\nlocation of the order\n\n\npartner\nStore or brand that provided the items requested by the client\n\n\npicking_time\nLead time that the clerk takes to gather all the items of the order.\n\n\ncashier_time\nTime take by the cashier to generate the bill\n\n\nrequesting_transport_time\nTime that the clerk takes to ask for a courier to pick up the basket.\n\n\nvehicle_type\nType of vehicle that the courier have\n\n\nstore_category\nMacro classification of the partner\n\n\nout_of_stock\nNumber of elements that were reported to be out of stock in the store.\n\n\norder_wrong\nIndicades if the order was sucessfully delivered or not\n\n\ntag_cancel\nIndicades if the order was cancelled or not.\n\n\ndistance_km\nTotal number of km that the delivery had from the partner to the client\n\n\nuse_credit\nIndicades if the order was payed with some promotion code\n\n\nhora\nHave the hour in which the order was issued\n\n\npolygon_size\nDefines the km of coverage of the partner\n\n\ncashback\nProvides the total amount of promotion code value in possess of the client\n\n\n\n\ndelivery.head(3)\n\n\n\n\n\n\n\n\ncountry\ndistrict_name\ndate\nweekday\nuser_total_orders\n% completed orders user\npayment_method\norder_value\nitem_count\ncity\n...\nvehicle_type\nstore_category\nout_of_stock\norder_wrong\ntag_cancel\ndistance_km\nuse_credit\nhora\npolygon_size\ncashback\n\n\n\n\n0\nLlaqta_1\nMA_1\n2023-06-30 00:00:00\nFriday\n111\n95.50 %\nrevol_pay\n25.407407\n2.0\nMaule\n...\nBicycle\nLicours Shop\n0\nFalse\nFalse\n2.0\nNaN\n23\nNaN\nNaN\n\n\n1\nLlaqta_1\nMA_2\n2023-06-27 00:00:00\nTuesday\n4\n50.00 %\ncc-N/A\n43.049383\n1.0\nMaule\n...\nMotorcycle\nE-commerce\n0\nTrue\nTrue\n3.0\nNaN\n0\nNaN\nNaN\n\n\n2\nLlaqta_1\nMA_3\n2023-06-27 00:00:00\nTuesday\n208\n99.04 %\ncc-credit\n119.438889\n11.0\nMaule\n...\nMotorcycle\nNaN\n0\nFalse\nFalse\n4.0\nNaN\n12\nNaN\nNaN\n\n\n\n\n3 rows × 24 columns\n\n\n\n\n# Checking null values\ndf.isnull().sum()\n\ncountry                          0\ndistrict_name                    0\nweekday                          0\norder_id                         0\nuser_total_orders                0\n% completed orders user          0\npayment_method                   0\norder_value                      0\nitem_count                     398\ncity                             0\npartner                          0\nlat                              2\nlng                              2\npicking_time                 11149\ncashier_time                  1191\nrequesting_transport_time    30258\nvehicle_type                   994\nstore_category               18125\nout_of_stock                     0\norder_wrong                      0\ntag_cancel                       0\ndistance_km                      2\nuse_credit                   54448\nhora                             0\npolygon_size                   226\ncashback                     54448\ndtype: int64\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 62382 entries, 0 to 62381\nData columns (total 26 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   country                    62382 non-null  object \n 1   district_name              62382 non-null  object \n 2   weekday                    62382 non-null  object \n 3   order_id                   62382 non-null  int64  \n 4   user_total_orders          62382 non-null  int64  \n 5   % completed orders user    62382 non-null  object \n 6   payment_method             62382 non-null  object \n 7   order_value                62382 non-null  float64\n 8   item_count                 61984 non-null  float64\n 9   city                       62382 non-null  object \n 10  partner                    62382 non-null  object \n 11  lat                        62380 non-null  float64\n 12  lng                        62380 non-null  float64\n 13  picking_time               51233 non-null  float64\n 14  cashier_time               61191 non-null  float64\n 15  requesting_transport_time  32124 non-null  float64\n 16  vehicle_type               61388 non-null  object \n 17  store_category             44257 non-null  object \n 18  out_of_stock               62382 non-null  int64  \n 19  order_wrong                62382 non-null  bool   \n 20  tag_cancel                 62382 non-null  bool   \n 21  distance_km                62380 non-null  float64\n 22  use_credit                 7934 non-null   object \n 23  hora                       62382 non-null  int64  \n 24  polygon_size               62156 non-null  float64\n 25  cashback                   7934 non-null   float64\ndtypes: bool(2), float64(10), int64(4), object(10)\nmemory usage: 11.5+ MB\n\n\n\ndf[['vehicle_type', 'store_category', 'distance_km', 'out_of_stock']].info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 62382 entries, 0 to 62381\nData columns (total 4 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   vehicle_type    61388 non-null  object \n 1   store_category  44257 non-null  object \n 2   distance_km     62380 non-null  float64\n 3   out_of_stock    62382 non-null  int64  \ndtypes: float64(1), int64(1), object(2)\nmemory usage: 1.9+ MB\n\n\n\n# Removing % label\ndf['% completed orders user']=df['% completed orders user'].str.replace('%',' ')\n\ndf['% completed orders user']=df['% completed orders user'].astype('float')\ndf['% completed orders user']=df['% completed orders user']/100\n\n\ndf[['% completed orders user']].info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 62382 entries, 0 to 62381\nData columns (total 1 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   % completed orders user  62382 non-null  float64\ndtypes: float64(1)\nmemory usage: 487.5 KB\n\n\n\n# Remove missing values\ndf.dropna(subset=['lat','lng','vehicle_type'],inplace=True)\n\n\n\n#Imputing values of mode and mean for the variables\n\nmode_ps = df['polygon_size'].mode()[0]\ndf['polygon_size'].fillna(mode_ps, inplace=True)\n\n\nmean_ic = df['item_count'].mean()\ndf['item_count'].fillna(mean_ic, inplace=True)\n\n\n\nmode_sc = df['store_category'].mode()[0]\ndf['store_category'].fillna(mode_sc, inplace=True)\n\n\n# Replace missing values\ndf.fillna({'t04_picking_time': 0, 't05_cashier_time': 0, 't06_requesting_rt_time':0, 'rappi_amount':0}, inplace=True)\n\n\n# Replace missing values\ndf.fillna({'use_credit': False},inplace=True)\n\n\n#$0 could be relate to bad order\nzero_count = (df['order_value_usd'] == 0).sum()\nprint(zero_count)\n\nKeyError: 'order_value_usd'\n\n\n\n# Check duplicate values\ndup_value = df.duplicated().sum()\nprint(dup_value)\n\n\n\ndf.isnull().sum()\n\n\ndf.describe()\n\n\ndf.head()\n\n\n# Removing negative values\nnegative_cols = ['user_total_orders', 'item_count', 't04_picking_time', 't05_cashier_time', 't06_requesting_rt_time',\n                 'out_of_stock', 'distance_km', 'hora', 'polygon_size', 'rappi_amount']\n\n# Create a mask that identifies rows with any negative value in the specified columns\nnegative_mask = df[negative_cols].lt(0).any(axis=1)\n\n# Use the mask to filter out rows with negative values\ndf1 = df[~negative_mask]\n\n\n# Separating quantiative & Categorical variables\ndf_cat = df.select_dtypes(include=['object','bool'])\ndf_quan = df.select_dtypes(exclude=['object','bool'])\n\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\ndf_quan_corr = df_quan.corr(method = 'spearman' )\nax = sns.heatmap(df_quan_corr, annot=True, cmap='coolwarm', center=0,fmt='.2g',xticklabels='auto', yticklabels='auto',linewidth=.8,cbar_kws={\"shrink\": .8})\nax.tick_params(axis='both', which='both', labelsize=8)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n# Create a pairplot\nsns.pairplot(df_quan)\n\n\nfor col in df_quan.columns:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\n    # Histogram\n    sns.histplot(df_quan[col], kde=True, bins=20, ax=ax[0])\n    ax[0].set_title(f'Histogram for {col}')\n    ax[0].set_xlabel(col)\n    ax[0].set_ylabel('Frequency')\n\n    # Box plot\n    sns.boxplot(x=df_quan[col], ax=ax[1])\n    ax[1].set_title(f'Boxplot for {col}')\n    ax[1].set_xlabel(col)\n\n    plt.tight_layout()\n    plt.show()\n\n\ndf_quan.columns\n\n\n#Removing outliers for : % completed orders user, picking_time, cashier_time, t_06, polygon size, rapi_amount\ncolumns = ['t04_picking_time', 't05_cashier_time',\n           't06_requesting_rt_time', 'polygon_size', 'rappi_amount']\n\nQ1 = df[columns].quantile(0.25)\nQ3 = df[columns].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = pd.DataFrame()\nfor col in columns:\n    condition = (df[col] &lt; lower_bound[col]) | (df[col] &gt; upper_bound[col])\n    outliers = pd.concat([outliers, df[condition]], ignore_index=True)\n\n\n#chi_squared for categorical\nfrom scipy.stats import chi2_contingency\n\nchi_squared_results = pd.DataFrame(columns=['Variable 1', 'Variable 2', 'Chi-Squared', 'P-Value'])\n\nfor var1 in df_cat:\n    for var2 in df_cat:\n        if var1 != var2:\n          #if var2 =='bad_order':\n            contingency_table = pd.crosstab(df[var1], df[var2])\n\n            chi2, p, _, _ = chi2_contingency(contingency_table)\n\n\n            new_row = pd.DataFrame({'Variable 1': [var1], 'Variable 2': [var2], 'Chi-Squared': [chi2], 'P-Value': [p]})\n\n            chi_squared_results = pd.concat([chi_squared_results, new_row], ignore_index=True)\n\nchi_squared_results\n\n\n#Checking for independance between our outcome bad_order and the categorical variables\n\n\n#chi_squared for categorical\nfrom scipy.stats import chi2_contingency\n\nchi_squared_results = pd.DataFrame(columns=['Variable', 'Chi-Squared', 'P-Value'])\n\noutcome_variable = 'bad_order'\n\nfor var in df_cat:\n        if var != outcome_variable:\n            contingency_table = pd.crosstab(df[outcome_variable], df[var])\n\n            chi2, p, _, _ = chi2_contingency(contingency_table)\n\n\n            new_row = pd.DataFrame({'Variable': var, 'Chi-Squared': [chi2], 'P-Value': [p]})\n\n            chi_squared_results = pd.concat([chi_squared_results, new_row], ignore_index=True)\n\n            if p &lt; 0.05:\n              print(f\"There is a significant association between {outcome_variable} and {var} (p-value = {p:.2f})\")\n            else:\n              print(f\"There is no significant association between {outcome_variable} and {var} (p-value = {p:.2f})\")\n\n#H0 there is no relathionship between the variables\n\nchi_squared_results\n\n\n#Perfoming ANOVA TEST for mixed categorical and numerical values\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\n# List of numerical variables for the ANOVA tests\ndf_cuantitative = ['% completed orders user', 'order_value_usd', 'item_count', 't04_picking_time', 't05_cashier_time', 't06_requesting_rt_time', 'out_of_stock', 'distance_km', 'hora']\n\n# Perform ANOVA for numerical variables\nfor num_var in df_cuantitative:\n    formula = f'{num_var} ~ C(bad_order)'\n    try:\n        model = ols(formula, data=df).fit()\n        anova_table = anova_lm(model)\n        print(f\"ANOVA table for {num_var} and bad_order:\")\n        print(anova_table)\n\n        p_value = anova_table['PR(&gt;F)'][0]\n        if p_value &lt; 0.05:\n            print(f\"There is a significant difference between the means of the groups defined by bad_order for {num_var} (p-value = {p_value:.2f})\")\n        else:\n            print(f\"There is no difference between the means of k groups\")\n\n        print(\"=\" * 40)\n\n    except Exception as e:\n        print(f\"Error for {num_var} and bad_order: {str(e)}\")"
  },
  {
    "objectID": "posts/delivery/Delivery_app_group_project.html#context",
    "href": "posts/delivery/Delivery_app_group_project.html#context",
    "title": "DELIVERY APP",
    "section": "",
    "text": "The next exercise is related to the development of a machine learning modeling that have as a goal to classify if an order is being delivered correctly or not.\nThe dataset provided is from a ficticious company that provides the service of delivery from stores as grocery shops, pharmacies and more to their clients. The company have as a SLA the commitment to delivery successfully all the orders that they have, if happens otherwise the company is penalized since they need to assume the losses of the order that have being handled poorly.\nThe company has a general understanding on the problems that lead to have unsusscesful order. For example:\n\nThe product requested has gone stockout and the partner did not report it on time.\nThe high demand creates that there are some lack of delivery vehicles in certain hours.\nThe products arrive to the clients in a badly state.\nThe route to deliver the order presents bottlenecks (stores, distance and more) and the order is not handled on time.\n\nNow, the company knows from where it comes the problems. However, wants to automatize the problem of recognizing orders that are in danger to being wrong handled. For it, it is displayed a dataset of orders, the characteristics that are being picked in real time about the order and the result of it.\nIn the article, it is going to be explained the process to provide of an algorithm to classify and recognize the orders in real time and issue aids to avoid the losses of wrong orders.\n\n# Displaying at least 50 columns\npd.set_option('display.max_columns',50)\n\nFirstly, the next libraries are going be imported in the lab. + pandas and numpy to manage the numerical data + seaborn and matplotlib are oriented to create statistical graphs\n\n# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n#/ @hidden_cell\ndelivery = pd.read_csv('.//ml_dlv.csv')\n\nData\nNow is going to be presented the data about a company that has as goal to deliver different products to the clients in the countries that it operates. In this table there is a lot of data about the operations logistic of the company. Many od thiese data comes is real time from several APIs scrateched from applications of the partners, the clients, and the couriers. Which now in this dataset have been tangled.\n\n\n\n\n\n\n\nField\nConcept\n\n\n\n\ncountry\nThere are going to three countries in the dataset\n\n\ndistrict_name\nName of the village in the city\n\n\ndate\nday when the order was commanded\n\n\nweekday\nname of the day when the order was commanded\n\n\nuser_total_orders\nnumber of previous orders that the client had ordered before\n\n\ncompleted orders user\nPercentage of the orders that actually were well delivered to the client\n\n\npayment_method\nDefines the way in which the client pays for the order\n\n\norder_value\nvalue of the order\n\n\nitem_count\nnumber of elements that the basket of the order have\n\n\ncity\nlocation of the order\n\n\npartner\nStore or brand that provided the items requested by the client\n\n\npicking_time\nLead time that the clerk takes to gather all the items of the order.\n\n\ncashier_time\nTime take by the cashier to generate the bill\n\n\nrequesting_transport_time\nTime that the clerk takes to ask for a courier to pick up the basket.\n\n\nvehicle_type\nType of vehicle that the courier have\n\n\nstore_category\nMacro classification of the partner\n\n\nout_of_stock\nNumber of elements that were reported to be out of stock in the store.\n\n\norder_wrong\nIndicades if the order was sucessfully delivered or not\n\n\ntag_cancel\nIndicades if the order was cancelled or not.\n\n\ndistance_km\nTotal number of km that the delivery had from the partner to the client\n\n\nuse_credit\nIndicades if the order was payed with some promotion code\n\n\nhora\nHave the hour in which the order was issued\n\n\npolygon_size\nDefines the km of coverage of the partner\n\n\ncashback\nProvides the total amount of promotion code value in possess of the client\n\n\n\n\ndelivery.head(3)\n\n\n\n\n\n\n\n\ncountry\ndistrict_name\ndate\nweekday\nuser_total_orders\n% completed orders user\npayment_method\norder_value\nitem_count\ncity\npartner\npicking_time\ncashier_time\nrequesting_transport_time\nvehicle_type\nstore_category\nout_of_stock\norder_wrong\ntag_cancel\ndistance_km\nuse_credit\nhora\npolygon_size\ncashback\n\n\n\n\n0\nLlaqta_1\nMA_1\n2023-06-30 00:00:00\nFriday\n111\n95.50 %\nrevol_pay\n25.407407\n2.0\nMaule\nBotillería Echenique\nNaN\n14187.0\nNaN\nBicycle\nLicores\n0\nFalse\nFalse\n2.0\nNaN\n23\nNaN\nNaN\n\n\n1\nLlaqta_1\nMA_2\n2023-06-27 00:00:00\nTuesday\n4\n50.00 %\ncc-N/A\n43.049383\n1.0\nMaule\nSOLOROSASYALGOMAS\n2.0\n0.0\nNaN\nMotorcycle\nFloristeria\n0\nTrue\nTrue\n3.0\nNaN\n0\nNaN\nNaN\n\n\n2\nLlaqta_1\nMA_3\n2023-06-27 00:00:00\nTuesday\n208\n99.04 %\ncc-credit\n119.438889\n11.0\nMaule\nJumbo\n8.0\n12.0\n1.0\nMotorcycle\nSuper\n0\nFalse\nFalse\n4.0\nNaN\n12\nNaN\nNaN"
  },
  {
    "objectID": "posts/delivery/Delivery_app_group_project.html#exploratory-data-analysis-eda-univariate-bivariate-analysis",
    "href": "posts/delivery/Delivery_app_group_project.html#exploratory-data-analysis-eda-univariate-bivariate-analysis",
    "title": "DELIVERY APP",
    "section": "2. Exploratory Data Analysis (EDA) (univariate / bivariate analysis)",
    "text": "2. Exploratory Data Analysis (EDA) (univariate / bivariate analysis)\n\n#chi_squared for categorical\nfrom scipy.stats import chi2_contingency\n\nchi_squared_results = pd.DataFrame(columns=['Variable 1', 'Variable 2', 'Chi-Squared', 'P-Value'])\n\nfor var1 in df_cat:\n    for var2 in df_cat:\n        if var1 != var2:\n          #if var2 =='bad_order':\n            contingency_table = pd.crosstab(df[var1], df[var2])\n\n            chi2, p, _, _ = chi2_contingency(contingency_table)\n\n\n            new_row = pd.DataFrame({'Variable 1': [var1], 'Variable 2': [var2], 'Chi-Squared': [chi2], 'P-Value': [p]})\n\n            chi_squared_results = pd.concat([chi_squared_results, new_row], ignore_index=True)\n\nchi_squared_results\n\n\n#Checking for independance between our outcome bad_order and the categorical variables\n\n\n#chi_squared for categorical\nfrom scipy.stats import chi2_contingency\n\nchi_squared_results = pd.DataFrame(columns=['Variable', 'Chi-Squared', 'P-Value'])\n\noutcome_variable = 'bad_order'\n\nfor var in df_cat:\n        if var != outcome_variable:\n            contingency_table = pd.crosstab(df[outcome_variable], df[var])\n\n            chi2, p, _, _ = chi2_contingency(contingency_table)\n\n\n            new_row = pd.DataFrame({'Variable': var, 'Chi-Squared': [chi2], 'P-Value': [p]})\n\n            chi_squared_results = pd.concat([chi_squared_results, new_row], ignore_index=True)\n\n            if p &lt; 0.05:\n              print(f\"There is a significant association between {outcome_variable} and {var} (p-value = {p:.2f})\")\n            else:\n              print(f\"There is no significant association between {outcome_variable} and {var} (p-value = {p:.2f})\")\n\n#H0 there is no relathionship between the variables\n\nchi_squared_results\n\n\n#Perfoming ANOVA TEST for mixed categorical and numerical values\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\n# List of numerical variables for the ANOVA tests\ndf_cuantitative = ['% completed orders user', 'order_value_usd', 'item_count', 't04_picking_time', 't05_cashier_time', 't06_requesting_rt_time', 'out_of_stock', 'distance_km', 'hora']\n\n# Perform ANOVA for numerical variables\nfor num_var in df_cuantitative:\n    formula = f'{num_var} ~ C(bad_order)'\n    try:\n        model = ols(formula, data=df).fit()\n        anova_table = anova_lm(model)\n        print(f\"ANOVA table for {num_var} and bad_order:\")\n        print(anova_table)\n\n        p_value = anova_table['PR(&gt;F)'][0]\n        if p_value &lt; 0.05:\n            print(f\"There is a significant difference between the means of the groups defined by bad_order for {num_var} (p-value = {p_value:.2f})\")\n        else:\n            print(f\"There is no difference between the means of k groups\")\n\n        print(\"=\" * 40)\n\n    except Exception as e:\n        print(f\"Error for {num_var} and bad_order: {str(e)}\")"
  },
  {
    "objectID": "posts/delivery/Delivery_app_group_project.html#data-cleaning-and-feature-engineering",
    "href": "posts/delivery/Delivery_app_group_project.html#data-cleaning-and-feature-engineering",
    "title": "DELIVERY APP",
    "section": "2. Data cleaning and feature engineering",
    "text": "2. Data cleaning and feature engineering\nNow that the data has been presented, the data team know beforehand that the the information provided can have some inconsistencies that comes from the system technology components. For example:\n\nRedundant data\nduplicate data\nmissing values\nmissleading data\n\nIn this section, we are going to cover some aspects to prepare the data to actually handle and initialize the machine learning modeling.\nData cleaning\nThe first check is to know which fields have null values. From which is noticed that: fields as item_count, the time related fields (for example picking time) and tag fields as polygon_size have a lot missing values. The most heavily cases are the next\n\ncashback: Have an 87% of missing values. It is justified since not all the clients have assigned an amount of money gifted by the company.\nuse_credit: Also 87% of missing values. Also related to the assumption of being assigned a gift amount of money to the client.\nrequesting_transport_time: This field needs to be reviewed, since every order needs to be assigned a value.\n\n\n# checking the percentage of missing values in the dataset\nmissing_percentage = (delivery.isna().sum() / len(delivery)) * 100\nmissing_percentage_sorted = missing_percentage.sort_values(ascending=False)\nprint(missing_percentage_sorted)\n\ncashback                     87.281588\nuse_credit                   87.281588\nrequesting_transport_time    48.504376\npicking_time                 17.872143\ncashier_time                  1.909205\nvehicle_type                  1.593408\nitem_count                    0.638005\npolygon_size                  0.362284\ndistance_km                   0.003206\nstore_category                0.000000\nhora                          0.000000\ndate                          0.000000\ntag_cancel                    0.000000\norder_wrong                   0.000000\nout_of_stock                  0.000000\n% completed orders user       0.000000\npayment_method                0.000000\nweekday                       0.000000\ndistrict_name                 0.000000\nuser_total_orders             0.000000\npartner                       0.000000\ncity                          0.000000\norder_value                   0.000000\ncountry                       0.000000\ndtype: float64\n\n\nTo handle this issues the next tasks are going to be executed: + For the categorical fields with critical missing values the mode is going to be assigned. + For the numerical fields with critical missing values the average is going to be put. Unless there are some business rules that explains the data. + For minor missing values fields, the rows with no data are going to be removed.\n\n# The field use_credit is a categorical field has several missing values because it is related to the fact\n#that the user do not used credit since this is the case, the blank values are filled with *False* \ndelivery.fillna({'use_credit': False},inplace=True)\n\n# Also, the numerical fields related to the lead time of the operation of order delivery have a lot of blank rows.\n#This can happen because the order didnt met the step or there was some internal issue in the technical side of the apps used\n# It is resolved to put the standard value of zero in all the cases.\ndelivery.fillna({'picking_time': 0, 'cashier_time': 0, 'requesting_transport_time':0, 'cashback':0}, inplace=True)\n\n# Remove missing values in the low missing blank impact fields. Since there might be problems in the tech side but can\n# ignored for the modeling\ndelivery.dropna(subset=['vehicle_type','polygon_size','item_count','distance_km'],inplace=True)\n\nWith the data cleaned of blank rows now it is time to check the validity of the data\n\nmissing_percentage = (delivery.isna().sum() / len(delivery)) * 100\nmissing_percentage_sorted = missing_percentage.sort_values(ascending=False) \nprint(missing_percentage_sorted)\n\ncountry                      0.0\ndistrict_name                0.0\npolygon_size                 0.0\nhora                         0.0\nuse_credit                   0.0\ndistance_km                  0.0\ntag_cancel                   0.0\norder_wrong                  0.0\nout_of_stock                 0.0\nstore_category               0.0\nvehicle_type                 0.0\nrequesting_transport_time    0.0\ncashier_time                 0.0\npicking_time                 0.0\npartner                      0.0\ncity                         0.0\nitem_count                   0.0\norder_value                  0.0\npayment_method               0.0\n% completed orders user      0.0\nuser_total_orders            0.0\nweekday                      0.0\ndate                         0.0\ncashback                     0.0\ndtype: float64\n\n\nThe field ‘% completed orders user’ is suposly to store percentage values. However, when checked the field it is being stored as a string.\n\ndelivery['% completed orders user'].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nIndex: 61386 entries, 0 to 62381\nSeries name: % completed orders user\nNon-Null Count  Dtype \n--------------  ----- \n61386 non-null  object\ndtypes: object(1)\nmemory usage: 959.2+ KB\n\n\nFor it, it is proposed a rearange on the field to convert it to float field.\n\n# Removing % label\n# first eliminate the string '%'\ndelivery['% completed orders user']=delivery['% completed orders user'].str.replace('%',' ')\n# define the field now as a float column\ndelivery['% completed orders user']=delivery['% completed orders user'].astype('float')\n# divide it by 100 to have the percentage value\ndelivery['% completed orders user']=delivery['% completed orders user']/100\n\nNow it will be checke the numercial values and see if the values provided are consitent and has a logic on it. For it it is going to be separated the dataset between numercial and categorical fields.\n\n# The fields defined as boolean and objects has string data in it. These are categorical data\ndf_cat = delivery.select_dtypes(include=['object','bool'])\n\n# The rest of fields contain numerical values\ndf_quan = delivery.select_dtypes(exclude=['object','bool'])\n\nNow we plot some histogram graphs to check how the data is distributed\n\nfor col in df_quan.columns:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\n    # Histogram\n    sns.histplot(df_quan[col], kde=True, bins=20, ax=ax[0])\n    ax[0].set_title(f'Histogram for {col}')\n    ax[0].set_xlabel(col)\n    ax[0].set_ylabel('Frequency')\n\n    # Box plot\n    sns.boxplot(x=df_quan[col], ax=ax[1])\n    ax[1].set_title(f'Boxplot for {col}')\n    ax[1].set_xlabel(col)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the graphs depicted some incongruences are being found:\n\n% completed orders: This field should only have values between 0 and 1 since it is a percentage. It is needed to eliminate the rare cases.\npicking time, cashier_time, requesting_transport_time: There are some high values in the field. this column has values in minutes. Many of them indicades days to process, which makes no sense.\nhora: The field has a range between 0 and 24. There are some ramdom values in it\npolygon size: Also random values are inside as 30 km, which makes a range bigger than the radious of a general city.\n\n\n# The issues recognized are being defined and filtered out\nmisspercentage = delivery['% completed orders user'] &gt; 1\n\n# eliminate the rows that have more than one day to be do. By business rule, this cases shouldnt happen\ndays_1 = delivery['picking_time'] &gt; 1440\ndays_2 = delivery['cashier_time'] &gt; 1440\ndays_3 = delivery['requesting_transport_time'] &gt; 1440\n\n#out of range 1 to 24 \nnohour = delivery['picking_time'] &gt; 24\n\n# set a maximum of polygon size as 10 km (extreme cases). Above it, are random cases\nsize = delivery['polygon_size'] &gt; 10\n\ndelivery_filtered = delivery[~(misspercentage | days_1 | days_2 | days_3 | nohour | size)]\ndelivery_filtered.head(10)\n\n\n\n\n\n\n\n\ncountry\ndistrict_name\ndate\nweekday\nuser_total_orders\n% completed orders user\npayment_method\norder_value\nitem_count\ncity\npartner\npicking_time\ncashier_time\nrequesting_transport_time\nvehicle_type\nstore_category\nout_of_stock\norder_wrong\ntag_cancel\ndistance_km\nuse_credit\nhora\npolygon_size\ncashback\n\n\n\n\n1\nLlaqta_1\nMA_2\n2023-06-27 00:00:00\nTuesday\n4\n0.5000\ncc-N/A\n43.049383\n1.0\nMaule\nSOLOROSASYALGOMAS\n2.0\n0.0\n0.0\nMotorcycle\nFloristeria\n0\nTrue\nTrue\n3.0\nFalse\n0\n3.0\n0.0\n\n\n2\nLlaqta_1\nMA_3\n2023-06-27 00:00:00\nTuesday\n208\n0.9904\ncc-credit\n119.438889\n11.0\nMaule\nJumbo\n8.0\n12.0\n1.0\nMotorcycle\nSuper\n0\nFalse\nFalse\n4.0\nFalse\n12\n3.0\n0.0\n\n\n4\nLlaqta_1\nMA_1\n2023-06-27 00:00:00\nTuesday\n455\n0.9692\ncc-N/A\n36.740741\n3.0\nMaule\nJardineria teresa jacqueline huenun espinoza\n0.0\n0.0\n0.0\nMotorcycle\nFloristeria\n0\nTrue\nTrue\n4.0\nFalse\n13\n3.0\n0.0\n\n\n6\nLlaqta_1\nMA_5\n2023-06-28 00:00:00\nWednesday\n9\n0.0000\ncash\n20.835802\n6.0\nMaule\nLider\n16.0\n5.0\n0.0\nBicycle\nSuper\n0\nTrue\nTrue\n1.0\nFalse\n19\n3.0\n0.0\n\n\n7\nLlaqta_1\nMA_6\n2023-06-27 00:00:00\nTuesday\n26\n0.9231\ncc-N/A\n33.666667\n1.0\nMaule\nGREEN GLASS MT\n1.0\n0.0\n0.0\nBicycle\nHogar\n0\nFalse\nFalse\n2.0\nFalse\n19\n3.0\n0.0\n\n\n8\nLlaqta_1\nMA_7\n2023-06-27 00:00:00\nTuesday\n5\n1.0000\ncash\n6.382716\n1.0\nMaule\nMgm Importaciones\n4.0\n0.0\n0.0\nCar\nHogar\n0\nTrue\nFalse\n3.0\nTrue\n21\n3.0\n8410.0\n\n\n10\nLlaqta_1\nMA_7\n2023-06-27 00:00:00\nTuesday\n303\n0.9505\ncc-N/A\n9.222222\n1.0\nMaule\nTecnofactory\n1.0\n0.0\n0.0\nBicycle\nTecnologia\n0\nFalse\nFalse\n6.0\nFalse\n9\n3.0\n0.0\n\n\n13\nLlaqta_1\nMA_6\n2023-06-26 00:00:00\nMonday\n138\n0.9710\ncc-N/A\n63.530864\n2.0\nMaule\nGPet MT\n0.0\n0.0\n0.0\nCar\nMascotas\n0\nFalse\nFalse\n6.0\nFalse\n15\n3.0\n0.0\n\n\n14\nLlaqta_1\nMA_3\n2023-06-27 00:00:00\nTuesday\n38\n1.0000\ncc-N/A\n83.839506\n3.0\nMaule\nPetvet (Evet)\n0.0\n53.0\n0.0\nCar\nMascotas\n0\nFalse\nFalse\n2.0\nFalse\n16\n3.0\n0.0\n\n\n15\nLlaqta_1\nMA_11\n2023-06-28 00:00:00\nWednesday\n148\n0.9730\ncc-N/A\n23.740741\n1.0\nMaule\nAnsaldo Toys\n0.0\n0.0\n0.0\nCar\nTecnologia\n0\nFalse\nFalse\n4.0\nFalse\n16\n3.0\n0.0\n\n\n\n\n\n\n\n\n# Create a heatmap\nplt.figure(figsize=(10, 8))\ndf_quan_corr = df_quan.corr(method = 'spearman' )\nax = sns.heatmap(df_quan_corr, annot=True, cmap='coolwarm', center=0,fmt='.2g',xticklabels='auto', yticklabels='auto',linewidth=.8,cbar_kws={\"shrink\": .8})\nax.tick_params(axis='both', which='both', labelsize=8)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n# Create a pairplot\nsns.pairplot(df_quan)\n\nC:\\Users\\equipo\\anaconda3\\Lib\\site-packages\\seaborn\\axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\ndf_quan.columns\n\n\n#Removing outliers for : % completed orders user, picking_time, cashier_time, t_06, polygon size, rapi_amount\ncolumns = ['t04_picking_time', 't05_cashier_time',\n           't06_requesting_rt_time', 'polygon_size', 'rappi_amount']\n\nQ1 = df[columns].quantile(0.25)\nQ3 = df[columns].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = pd.DataFrame()\nfor col in columns:\n    condition = (df[col] &lt; lower_bound[col]) | (df[col] &gt; upper_bound[col])\n    outliers = pd.concat([outliers, df[condition]], ignore_index=True)"
  }
]