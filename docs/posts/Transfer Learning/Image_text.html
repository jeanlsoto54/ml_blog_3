<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Jean Luis Soto’s Portfolio - Generative AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jean Luis Soto’s Portfolio</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jeanlsoto54"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/jeanluissoto/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Generative AI</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#about" id="toc-about" class="nav-link active" data-scroll-target="#about">About</a>
  <ul class="collapse">
  <li><a href="#capture-images" id="toc-capture-images" class="nav-link" data-scroll-target="#capture-images">Capture images</a></li>
  <li><a href="#detecting-objects" id="toc-detecting-objects" class="nav-link" data-scroll-target="#detecting-objects">Detecting objects</a></li>
  <li><a href="#generative-ai-llm" id="toc-generative-ai-llm" class="nav-link" data-scroll-target="#generative-ai-llm">Generative AI: LLM</a></li>
  <li><a href="#final-result" id="toc-final-result" class="nav-link" data-scroll-target="#final-result">Final result</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="GenAI_Final_Project-preview.html"><i class="bi bi-journal-code"></i>Final Project Generative AI</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="about" class="level1">
<h1>About</h1>
<p>In this article it is going to be explained with a real case the use of the Generative AI. This type of artificial intelligence can create new content or models based on the data it is being trained on. Inside the architectures of this kind of models are supported with Neural Network components. This type of AI can create images, videos texts, speechs. The most well known example of this technology is in the field of Large Language Models (LLM) with <strong>GPT-4</strong></p>
<p>For this use case, the objective in this work is to achieve an application where it receives data in the form of images/video and the model recognize the objects in it and provides information from an LLM.</p>
<p>To achieve this the pipeline proposed is:</p>
<ol type="1">
<li><p>Define the code to open the camera’s device and obtain a photo frame.</p></li>
<li><p>Search for a Deep Learning model that works on object classfication and apply it on the image.</p></li>
<li><p>Obtain an Open Generative AI related to LLM and define the use of it.</p></li>
<li><p>Ensemble the models and make it function all together.</p></li>
</ol>
<section id="capture-images" class="level2">
<h2 class="anchored" data-anchor-id="capture-images">Capture images</h2>
<p>At first it is going to be imported necessary libraries to mnipulate image data</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the libraries related to image and video</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  module Python Imaging Library:</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the module Image represents an image object</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the module ImageDraw provide functions to draw lines,text </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and shapes in images</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image,ImageDraw</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Computer vision library</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#torchvision is a library that can handle datasets transforms #from computer vision tasks</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection <span class="im">import</span> fasterrcnn_resnet50_fpn</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the next chunk of code it is being activated the camera in a pop-up window. This module has two options <strong>s</strong> to capture the frame and save it and <strong>q</strong> to stop the module.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>cap <span class="op">=</span> cv2.VideoCapture(<span class="dv">0</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    ret, frame <span class="op">=</span> cap.read()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> ret: </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    cv2.imshow(<span class="st">'Energy Consuption task'</span>, frame)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    key <span class="op">=</span> cv2.waitKey(<span class="dv">1</span>) <span class="op">&amp;</span> <span class="bn">0xFF</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> key <span class="op">==</span> <span class="bu">ord</span>(<span class="st">'s'</span>):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        current_time <span class="op">=</span> time.strftime(<span class="st">"%Y%m</span><span class="sc">%d</span><span class="st">%H%M%S"</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Saving Image "</span> <span class="op">+</span> current_time <span class="op">+</span> <span class="st">".jpg"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">file</span> <span class="op">=</span> <span class="st">"./"</span> <span class="op">+</span> current_time <span class="op">+</span> <span class="st">".jpg"</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="bu">file</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        cv2.imwrite(<span class="bu">file</span>, frame)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> key <span class="op">==</span> <span class="bu">ord</span>(<span class="st">'q'</span>):</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>cap.release()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>cv2.destroyAllWindows()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here is shown the result of the previous chunk of code.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-result-1" class="cell" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<figure class="figure">
<p><img src="Image_text_files/figure-html/GenAI_Final_Project-result-1-output-1.png" id="result-1" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="detecting-objects" class="level2">
<h2 class="anchored" data-anchor-id="detecting-objects">Detecting objects</h2>
<p>For the task of detecting objects in images it is going to be used the concept of <em>Transfer Learning</em>: technique in machine learning where a pre-trained model is used as a starting point for solving a problem. In this case, the search for a pretrained Image Classification model is being search in <strong>Hugging Face</strong> library. a renown company that provides several open-source libraries and models to be used. The pre-trained model chosen is <a href="https://huggingface.co/Theem/fasterrcnn_resnet50_fpn_grayscale"><strong>Faster R-CNN</strong></a></p>
<p>Which was trained on the <a href="https://paperswithcode.com/dataset/coco"><strong>COCO</strong></a> (Common Objects in Context) dataset which posses around 80 objects categories as person, car, airplane and many other common objects.</p>
<p>With the code defined next is being uploaded the referred model in the workstation. <em>Pretrained</em> option is giving the command to keep the pretrained weights to classify obtained in the model. Later, this option can be changed depending on the context of the goal that is wanted to achieve. Some issue that can happen is that is needed to include new categories to detect in the model. In this case a prompting in the pre-trained model can be done by retraining to the desired result.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> fasterrcnn_resnet50_fpn(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, in being defined the function <em>detect_objects</em>. This function need as an input an image, which it is going to be converted to a tensor and after it the object detection model is executed and as output is fetched the probabilities of the objects detected.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detect_objects(image):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor()])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Preprocess the image</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    input_image <span class="op">=</span> transform(image).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform inference</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> model(input_image)[<span class="dv">0</span>]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predictions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The function created is being tested on the image captured before and is printed the results of the model:</p>
<ul>
<li>boxes: have the position of the detected object</li>
<li>labels: define the class detected per object</li>
<li>scores: provides the % confidence of the object</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>testing_image_path <span class="op">=</span> <span class="st">'path/photo_frame_to_analyze.jpg'</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>testing_image <span class="op">=</span> Image.<span class="bu">open</span>(testing_image_path).convert(<span class="st">"RGB"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform object detection on the testing image</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>detections <span class="op">=</span> detect_objects(testing_image)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the detection results</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(detections)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-embed-nb-cell">
<div id="result-5" class="cell" data-execution_count="7">
<div class="cell-output cell-output-stdout">
<pre><code>{'boxes': tensor([[3.2203e-01, 4.1001e+02, 1.9834e+01, 4.7844e+02],
        [1.4888e+02, 4.1843e+02, 1.9023e+02, 4.7068e+02],
        [1.6601e+01, 2.1219e+02, 2.5453e+01, 2.6549e+02],
        [3.0753e+01, 2.1413e+02, 4.0715e+01, 2.6554e+02],
        [2.3527e+01, 2.1504e+02, 3.2301e+01, 2.6518e+02],
        [0.0000e+00, 2.0175e+02, 6.4201e+01, 2.6924e+02],
        [1.2589e+02, 4.2713e+02, 1.3912e+02, 4.7381e+02],
        [4.4964e+01, 2.0565e+02, 5.3511e+01, 2.6715e+02],
        [1.1428e+02, 4.2024e+02, 1.3183e+02, 4.7452e+02],
        [5.0927e+01, 2.0562e+02, 6.1833e+01, 2.6271e+02],
        [1.0185e+01, 2.1170e+02, 2.7680e+01, 2.6598e+02],
        [2.5588e+01, 2.1307e+02, 4.4688e+01, 2.6691e+02],
        [5.5887e+00, 2.1109e+02, 4.2185e+01, 2.6668e+02],
        [3.9336e+01, 2.1138e+02, 4.6657e+01, 2.6600e+02],
        [3.2911e+02, 3.5286e+02, 3.4648e+02, 3.7866e+02],
        [8.6915e+00, 2.1093e+02, 1.7139e+01, 2.6546e+02],
        [1.2237e+02, 4.2523e+02, 1.3136e+02, 4.7357e+02],
        [4.7442e+01, 4.6254e+02, 8.0911e+01, 4.7838e+02],
        [9.8479e-01, 2.0925e+02, 6.2978e+00, 2.6621e+02],
        [3.7348e+01, 2.1006e+02, 5.3997e+01, 2.6665e+02],
        [2.0104e+01, 2.1301e+02, 2.7574e+01, 2.6499e+02],
        [3.5595e+02, 4.5401e+02, 4.0307e+02, 4.6278e+02],
        [1.3089e+02, 4.2820e+02, 1.4375e+02, 4.6783e+02],
        [6.7619e+01, 4.3832e+02, 9.0605e+01, 4.4558e+02],
        [1.0059e+02, 3.8950e+02, 1.2099e+02, 4.1813e+02],
        [1.2107e+01, 2.1057e+02, 2.0228e+01, 2.6547e+02],
        [4.2306e+01, 2.0949e+02, 4.9903e+01, 2.6626e+02],
        [3.5642e+02, 4.3070e+02, 3.9371e+02, 4.4585e+02],
        [1.4157e+01, 1.6911e+02, 7.0233e+01, 1.9584e+02],
        [1.1285e+02, 4.2322e+02, 1.2332e+02, 4.7522e+02],
        [3.2599e+00, 2.0830e+02, 9.6703e+00, 2.6741e+02],
        [4.0722e+01, 2.0423e+02, 6.4438e+01, 2.6618e+02],
        [6.9234e+01, 4.5592e+02, 1.1075e+02, 4.7433e+02],
        [4.8032e+01, 2.0489e+02, 5.7456e+01, 2.6485e+02],
        [2.3738e+01, 2.0776e+02, 5.8403e+01, 2.6732e+02],
        [3.5193e+02, 4.1967e+02, 3.9584e+02, 4.4932e+02],
        [6.2612e+01, 4.0642e+02, 8.7142e+01, 4.3817e+02],
        [6.2946e+00, 2.1037e+02, 1.3053e+01, 2.6569e+02],
        [3.5634e+02, 4.5140e+02, 4.0120e+02, 4.5843e+02],
        [2.6891e+02, 4.3979e+02, 2.9758e+02, 4.6742e+02],
        [2.3334e+02, 2.5023e+02, 2.3808e+02, 2.5447e+02],
        [1.3672e+02, 4.2808e+02, 1.4695e+02, 4.5387e+02],
        [6.0978e+01, 4.5818e+02, 8.2778e+01, 4.7626e+02],
        [6.7801e+01, 4.5512e+02, 1.1142e+02, 4.6574e+02],
        [6.3500e+01, 4.5738e+02, 8.5448e+01, 4.6682e+02],
        [1.5077e+02, 4.1576e+02, 2.4865e+02, 4.7335e+02],
        [3.5520e+02, 4.4956e+02, 4.0018e+02, 4.5586e+02],
        [9.8056e+01, 4.2487e+02, 1.2711e+02, 4.7433e+02],
        [1.1894e+02, 4.2531e+02, 1.5298e+02, 4.7410e+02],
        [7.2687e+01, 4.3941e+02, 1.1587e+02, 4.7407e+02],
        [1.2256e+02, 4.1754e+02, 1.8561e+02, 4.7512e+02],
        [9.8189e+01, 3.8443e+02, 1.2473e+02, 4.2096e+02],
        [3.5495e+02, 4.3953e+02, 3.9407e+02, 4.4995e+02],
        [7.6286e+00, 4.1355e+02, 4.8398e+01, 4.7839e+02],
        [1.1270e+00, 1.8263e+02, 6.0701e+01, 2.0107e+02],
        [1.3181e+02, 4.2388e+02, 1.7081e+02, 4.6949e+02],
        [3.6750e+02, 4.0903e+02, 3.8020e+02, 4.2882e+02],
        [5.9032e+01, 2.0605e+02, 6.8070e+01, 2.6710e+02],
        [2.3341e+02, 2.5021e+02, 2.3812e+02, 2.5392e+02],
        [1.5393e+02, 4.1460e+02, 2.6840e+02, 4.7516e+02],
        [1.3093e+02, 4.2669e+02, 1.5072e+02, 4.5697e+02],
        [3.2243e+02, 4.6083e+02, 4.3508e+02, 4.7982e+02],
        [3.6648e+02, 4.1095e+02, 3.8145e+02, 4.2896e+02],
        [6.4154e+01, 4.6018e+02, 8.5638e+01, 4.7124e+02],
        [8.7079e+01, 4.3876e+02, 1.1470e+02, 4.4897e+02],
        [1.4477e+02, 4.5683e+02, 1.7427e+02, 4.7161e+02],
        [1.4931e+02, 4.1233e+02, 2.7706e+02, 4.7614e+02],
        [5.6233e+01, 4.3530e+02, 1.4235e+02, 4.7420e+02],
        [2.2881e+02, 4.4564e+02, 2.6855e+02, 4.7277e+02],
        [7.8557e-01, 1.6484e+02, 4.8610e+01, 1.9153e+02],
        [3.2307e+02, 4.6261e+02, 4.3285e+02, 4.8000e+02]]), 'labels': tensor([62, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 10, 84, 84, 75,
        84, 84, 84, 84, 84, 84,  1, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,
        64, 84, 84,  3, 38, 84, 75, 84, 75, 84, 84, 84, 84, 84, 84, 85, 84, 62,
        84, 84, 44, 84, 16, 72, 84, 33,  1, 75, 84, 84,  8, 84,  3, 84, 15]), 'scores': tensor([0.9074, 0.8842, 0.8691, 0.8563, 0.8438, 0.8296, 0.8295, 0.7874, 0.7130,
        0.6751, 0.6579, 0.6506, 0.6096, 0.5842, 0.5737, 0.5661, 0.5087, 0.4953,
        0.4884, 0.4865, 0.4818, 0.4619, 0.4596, 0.4568, 0.4559, 0.4539, 0.4456,
        0.3914, 0.3834, 0.3728, 0.3693, 0.3603, 0.3183, 0.3048, 0.2935, 0.2692,
        0.2603, 0.2496, 0.2142, 0.1946, 0.1760, 0.1689, 0.1540, 0.1540, 0.1428,
        0.1154, 0.1131, 0.0974, 0.0935, 0.0855, 0.0840, 0.0820, 0.0815, 0.0811,
        0.0794, 0.0787, 0.0757, 0.0750, 0.0746, 0.0739, 0.0718, 0.0698, 0.0694,
        0.0689, 0.0646, 0.0641, 0.0554, 0.0552, 0.0533, 0.0525, 0.0513])}</code></pre>
</div>
</div>
</div>
</section>
<section id="generative-ai-llm" class="level2">
<h2 class="anchored" data-anchor-id="generative-ai-llm">Generative AI: LLM</h2>
<p>The next step in the task is to obtain another pre-trained model, in this case a <strong>Generative AI</strong> model which can help us in the task of providing descriptions on the objects detected in the image frames. For this step, is going to be used to powerfull libraries: + LangChain: lightweight framework meant to make it easier to integrate and orchestaste LLM within application. + HuggingFace Hub: This hub possess many open-source LLMs.It can be access via token</p>
<p>In the next code is set an object <em>secret</em> saved in a .env file that holds the token to access to the hub. With this API connection is being saved a lot of ram space, since the LLM models possess many features (more than a million) which is also translated in heavy data models.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>dotenv_path <span class="op">=</span> Path(<span class="st">'./secret.env'</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>load_dotenv(dotenv_path<span class="op">=</span>dotenv_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now that the notebook is connected to the Hub, it’s being selected the next LLM: <a href="https://huggingface.co/tiiuae/falcon-7b-instruct"><strong>Falcon 7b Instruct</strong></a>. Which is ready-to-use chat model based on <em>Falcon-7b</em>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> langchain <span class="im">import</span> HuggingFaceHub</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>repo_id <span class="op">=</span> <span class="st">'tiiuae/falcon-7b-instruct'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As example, is send a generic question to the LLM: “What is a laptop?”. Given us an aswer to it.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-result-2" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> HuggingFaceHub(repo_id<span class="op">=</span>repo_id) </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>llm(<span class="st">"What is a laptop?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="result-2" class="cell-output cell-output-display" data-execution_count="4">
<pre><code>'What is a laptop?\nA laptop is a portable computer that is typically smaller than a desktop computer. It is designed to be used on the go and can be used for a variety of tasks, including browsing the internet, working on documents, and playing games.'</code></pre>
<p>Result of the LLM to a generic question</p>
</div>
</div>
</div>
<p>Now in the function <em>llm</em> is being called the LLM model. to start this function is needed as an input a text which a question. After it is going to be fetched the result and the resulting text is going to be splitted to discard the part of the question and only keeping the answer. There is some configurations set in the LLM model + temperature: this feature control the randomness of the answers provided by the llm. being number close to 1 related to the creative answers and the ones next to 0 being more controlled answers. + max_lenght: it is being set a default lenght of response to not overload the images with too many texts</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> llm(prompt):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    repo_id <span class="op">=</span> <span class="st">"tiiuae/falcon-7b-instruct"</span> </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    llm <span class="op">=</span> HuggingFaceHub(repo_id<span class="op">=</span>repo_id, model_kwargs<span class="op">=</span>{<span class="st">"temperature"</span>: <span class="fl">0.5</span>, <span class="st">"max_length"</span>: <span class="dv">3</span>})</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> llm(prompt)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    answer <span class="op">=</span> answer.split(<span class="st">"</span><span class="ch">\n</span><span class="st">A"</span>)[<span class="dv">1</span>].strip()</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> answer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This next code is validating the way that the question is going to be send to the model: generic question + object detected on the image.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>object_name <span class="op">=</span> <span class="st">'flask'</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>llm(<span class="ss">f'can you tell me what is a  </span><span class="sc">{</span>object_name<span class="sc">}</span><span class="ss">?'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="final-result" class="level2">
<h2 class="anchored" data-anchor-id="final-result">Final result</h2>
<p>The final chunk of code is going to ensemble the previous steps to appy descriptions to the objects found in the image. for it is send the initial image. After it, this image is converted to tensor and is being calculated the detection of objects. To avoid some missclassifications is being filtered only the predictions that are above 85% of probability. After it is used the <em>boxes</em> positions to draw a squere where the object is found. Only on the objects detected is run the <em>llm</em> function and this text is send as a title in the image.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#path of the input image</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>testing_path <span class="op">=</span> <span class="st">'./image_input.jpg'</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">#path of the image changed</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>output_path <span class="op">=</span><span class="st">'./image_modified_with_LLM_text.jpg'</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">#put the image in a 3 channel object: RGB</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>testing <span class="op">=</span> Image.<span class="bu">open</span>(testing_path).convert(<span class="st">'RGB'</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">#detecting all the objects in the image</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>detecting <span class="op">=</span> detect_objects(testing)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>draw <span class="op">=</span> ImageDraw.Draw(testing)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> box, label, score <span class="kw">in</span> <span class="bu">zip</span>(detecting[<span class="st">'boxes'</span>], detecting[<span class="st">'labels'</span>], detecting[<span class="st">'scores'</span>]):</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#checking that the objects above the 85% probability are selected and on it draw the box</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and run the llm function to describe the object</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> score <span class="op">&gt;</span> <span class="fl">0.85</span>:</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        draw.rectangle([(box[<span class="dv">0</span>], box[<span class="dv">1</span>]), (box[<span class="dv">2</span>], box[<span class="dv">3</span>])], outline<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> label.item() <span class="kw">in</span> coco_classes:  <span class="co"># Check if label exists in coco_classes</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            description <span class="op">=</span> llm(<span class="ss">f"what is a </span><span class="sc">{</span>coco_classes[label.item()]<span class="sc">}</span><span class="ss">?"</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>            draw.text((box[<span class="dv">0</span>], box[<span class="dv">1</span>]), <span class="ss">f"Label: </span><span class="sc">{</span>coco_classes[label.item()]<span class="sc">}</span><span class="ss">, Description: </span><span class="sc">{</span>description<span class="sc">}</span><span class="ss">"</span>, fill<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="co">#saving the resulting object</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>testing.save(output_path)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>testing.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The result is the next, where it is only found the object book, which is well classified and is describing the object thanks to the LLM used.</p>
<div class="quarto-embed-nb-cell">
<div id="cell-result-3" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>Image.<span class="bu">open</span>(output_path)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Image_text_files/figure-html/GenAI_Final_Project-result-3-output-1.png" class="img-fluid figure-img"></p>
<figcaption>Image with the description of the object detected</figcaption>
</figure>
</div>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>